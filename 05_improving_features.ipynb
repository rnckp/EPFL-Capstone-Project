{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2de47a-247b-4882-9f01-88324dbe4148",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535ff37a-91d9-40a0-b17e-251e18765f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import FuncFormatter, PercentFormatter\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "COLOR_FEMALE = cm.get_cmap(\"RdBu\")(220) # set blue for female\n",
    "COLOR_MALE = cm.get_cmap(\"RdBu\")(35) # set red for male\n",
    "TITLE_SIZE = 22\n",
    "TITLE_PADDING = 10\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "params = {\n",
    "    'text.color': (0.25, 0.25, 0.25),\n",
    "    'figure.figsize': [12, 5],\n",
    "   }\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 500\n",
    "\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "np.random.seed(42)\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from shutil import copyfile\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59a84a3-cf6e-4dfc-b3c8-5e5c4e8fda64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, PowerTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import umap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39cc8d3-cf0a-48db-9772-c3e4ae1ae7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import pydub\n",
    "from pydub import AudioSegment, effects\n",
    "\n",
    "from aubio import source, pitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaaa2df-0bac-491f-993a-9e115d5a4733",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80fd684e-f4c5-43ec-9b9b-378bd8c6be0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_contributors(data, max_samples_per_contributor=100):\n",
    "    # I set max_samples_per_contributor=100 because this is the average contribution \n",
    "    # per male client_id (vs just 14 for women).\n",
    "    # I aim for a filtered data set which is as varied as possible and \n",
    "    # I try minimize the skew towards heavy contributors.\n",
    "    g = data.groupby(\"client_id\")\n",
    "    tmp = []\n",
    "    for _, data in g:\n",
    "        if len(data) > max_samples_per_contributor:\n",
    "            tmp.append(data.sample(max_samples_per_contributor, random_state=42))\n",
    "        else:\n",
    "            tmp.append(data)\n",
    "    tmp = pd.concat(tmp)\n",
    "#     tmp = tmp.reset_index(drop=True)\n",
    "    return tmp\n",
    "    \n",
    "    \n",
    "def prepare_metadata(df_, language):\n",
    "    # drop all samples where label for target variable is missing\n",
    "    df_.dropna(subset=[\"gender\"], inplace=True)\n",
    "\n",
    "    # simplify client_id\n",
    "    mapping = dict(zip(df_.client_id.unique(), range(df_.client_id.nunique())))\n",
    "    df_.client_id = df_.client_id.map(mapping)\n",
    "\n",
    "    # map textual values for age to integers\n",
    "    ages = ['teens', 'twenties', 'thirties', 'fourties', 'fifties', 'sixties',\n",
    "           'seventies', 'eighties', 'nineties']\n",
    "    ages_int = range(10, 100, 10)\n",
    "    mapping = dict(zip(ages, ages_int))\n",
    "    df_.age = df_.age.map(mapping)\n",
    "\n",
    "    # get word_count as rough measure of recording length\n",
    "    df_[\"word_count\"] = df_.sentence.apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    # drop features that that I do not need\n",
    "    df_.drop([\"sentence\", \"segment\", \"locale\", \"up_votes\", \"down_votes\"], axis=1, inplace=True)\n",
    "\n",
    "    # drop gender «other» because ambiguous what that entails\n",
    "    to_drop = df_[df_.gender==\"other\"].index\n",
    "    df_.drop(to_drop, inplace=True)\n",
    "    \n",
    "    # remove samples with three words or less\n",
    "    to_drop = df_[df_.word_count<=3].index\n",
    "    df_.drop(to_drop, inplace=True)\n",
    "\n",
    "    # drop recordings from teens because gender in that age is hard to distinguish\n",
    "    to_drop = df_[df_.age==10].index\n",
    "    df_.drop(to_drop, inplace=True)\n",
    "    \n",
    "    df_.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # split male and female\n",
    "    female = df_[df_.gender==\"female\"].copy()\n",
    "    male = df_[df_.gender==\"male\"].copy()\n",
    "    \n",
    "    # reduce female samples to a max count of 100 per contributor to reduce bias towards heavy contributors\n",
    "    female = sample_contributors(female, max_samples_per_contributor=100)\n",
    "    male = sample_contributors(male, max_samples_per_contributor=100)\n",
    "\n",
    "    # sample from men as many samples as I have female samples\n",
    "    # again make sure that I include all voices and reduce bias towards men that have contributed heavily\n",
    "    sampled_ids = []\n",
    "    while len(sampled_ids)<len(female):\n",
    "        idx = male.groupby(\"client_id\").sample(1).index\n",
    "        sampled_ids.extend(idx)\n",
    "        male.drop(idx, inplace=True)\n",
    "    male = df_.iloc[sampled_ids].copy()\n",
    "    # reduce male samples to exact the sample size that we have for females\n",
    "    male = male.sample(len(female), random_state=42)\n",
    "    df_ = pd.concat([male, female])\n",
    "        \n",
    "    # add boolean for gender\n",
    "    df_[\"man\"] = np.where(df_.gender==\"male\", True, False)\n",
    "    \n",
    "    # reduce to essential columns\n",
    "    cols = [\"client_id\", \"path\", \"gender\", \"man\"]\n",
    "    df_ = df_[cols]\n",
    "\n",
    "    # change path to filename (which it rather is) and add actual filepath according to language\n",
    "    df_.rename({\"path\":\"file_name\"}, axis=1, inplace=True)\n",
    "    base_path = f\"_data/CommonVoice/{language}/clips/\"\n",
    "    df_[\"file_path\"] = base_path + df_.file_name\n",
    "    \n",
    "    # reorder for aestethics ;0)\n",
    "    cols = ['client_id', 'gender', 'man', 'file_name', 'file_path']\n",
    "    df_ = df_[cols]\n",
    "    \n",
    "    df_.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef50d3a-2b87-4a50-8a9c-58863c498693",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE        = 48_000\n",
    "MFCC_MIN_FREQUENCY = 60\n",
    "MFCC_MAX_FREQUENCY = 8_000\n",
    "MFCC_BANDS         = 80\n",
    "FO_CUTOFF_UPPER    = 1_500\n",
    "HOP_LENGTH         = 512\n",
    "\n",
    "\n",
    "def get_fO(audio, sample_rate):\n",
    "    # create empty array to return in case processing fails\n",
    "    empty_ = np.empty(5,)\n",
    "    empty_[:] = np.nan\n",
    "\n",
    "    splits_cnt = int(len(audio)/HOP_LENGTH)\n",
    "    splits = [audio[x * HOP_LENGTH:x * HOP_LENGTH + HOP_LENGTH] for x in range(0, splits_cnt)]\n",
    "    \n",
    "    # \"yinfast\" and \"yin\" both work fine and deliver almost identical results, yinfast being around 2x faster \n",
    "    # \"yinfft\" yields many NaNs\n",
    "    pitch_o = pitch(\"yinfast\", samplerate=sample_rate)\n",
    "    pitches = []\n",
    "    for split in splits:\n",
    "        pitch_ = pitch_o(split)[0]\n",
    "        confidence = pitch_o.get_confidence()\n",
    "        if confidence < 0.8: \n",
    "            pitch_ = 0.\n",
    "        pitches.append(pitch_) \n",
    "\n",
    "    if len(pitches)==0:\n",
    "        return empty_\n",
    "\n",
    "    fO = [0.0 if x>FO_CUTOFF_UPPER else x for x in pitches]\n",
    "    fO = [x for x in fO if x!=0.0]\n",
    "\n",
    "    if len(fO)==0:\n",
    "        return empty_ \n",
    "\n",
    "    return (np.mean(fO), np.median(fO), np.std(fO), np.min(fO), np.max(fO))\n",
    "\n",
    "\n",
    "def pydub_to_np(audio):\n",
    "    return np.array(audio.get_array_of_samples(), dtype=np.float32).reshape((-1, audio.channels)).T\n",
    "\n",
    "\n",
    "def detect_leading_silence(audio, silence_threshold=-50.0, chunk_size=10):\n",
    "    trim_ms = 0 # ms\n",
    "    assert chunk_size > 0 # to avoid infinite loop\n",
    "    while audio[trim_ms:trim_ms+chunk_size].dBFS < silence_threshold and trim_ms < len(audio):\n",
    "        trim_ms += chunk_size\n",
    "    return trim_ms\n",
    "\n",
    "\n",
    "def get_features(file, secs_to_process=2):\n",
    "    # create empty array to return in case processing fails\n",
    "    empty_ = np.empty(85,)\n",
    "    empty_[:] = np.nan \n",
    "    \n",
    "    audio_file = AudioSegment.from_file(file)\n",
    "    sample_rate = audio_file.frame_rate\n",
    "\n",
    "    # normalize audio levels\n",
    "    try:\n",
    "        audio_file = effects.normalize(audio_file)  \n",
    "    except Exception as e:\n",
    "        print(f\"{file} can't be normalized\")\n",
    "        return empty_  \n",
    "    \n",
    "    # remove silence at beginning and end\n",
    "    start_trim = detect_leading_silence(audio_file)\n",
    "    end_trim = detect_leading_silence(audio_file.reverse())\n",
    "    audio_duration = len(audio_file) \n",
    "    audio_file = audio_file[start_trim:audio_duration-end_trim]\n",
    "    \n",
    "    if audio_file.duration_seconds > secs_to_process:\n",
    "        ms_to_process = secs_to_process*1000\n",
    "        center_of_audio = len(audio_file)/2 \n",
    "        start_extract = center_of_audio - ms_to_process/2 \n",
    "        end_extract = start_extract + ms_to_process\n",
    "        audio_file = audio_file[start_extract:end_extract]\n",
    "        \n",
    "    waveform = pydub_to_np(audio_file)\n",
    "\n",
    "    if waveform.ndim == 2: # check if file is stereo \n",
    "        waveform = waveform.mean(axis=0) # if stereo merge channels by averaging\n",
    "\n",
    "    if len(waveform)>0:\n",
    "        fO_ = get_fO(waveform, sample_rate)\n",
    "        mfcc_ = np.mean(librosa.feature.mfcc(y=waveform, \n",
    "                                             sr=sample_rate, \n",
    "                                             n_mfcc=MFCC_BANDS, \n",
    "                                             fmin=MFCC_MIN_FREQUENCY,\n",
    "                                             fmax=MFCC_MAX_FREQUENCY,\n",
    "                                            ), axis=1) \n",
    "        return np.hstack([fO_, mfcc_])\n",
    "    \n",
    "    else:\n",
    "        return empty_    \n",
    "\n",
    "    \n",
    "def get_cols():\n",
    "    mel_to_Hz = librosa.mel_frequencies(n_mels=MFCC_BANDS, \n",
    "                                        fmin=MFCC_MIN_FREQUENCY, \n",
    "                                        fmax=MFCC_MAX_FREQUENCY)\n",
    "    \n",
    "    cols = [\"fO_mean\", \"fO_median\", \"fO_std\", \"fO_min\", \"fO_max\"]\n",
    "    cols_mel = [f\"{x:.0f}\" + \"_Hz\" for x in mel_to_Hz]\n",
    "    cols.extend(cols_mel)\n",
    "    return cols\n",
    "    \n",
    "    \n",
    "def process_audio_files(file_paths, secs_to_process=2):\n",
    "    features = []\n",
    "    for file_path in tqdm(file_paths):\n",
    "        features.append(get_features(file_path, secs_to_process=secs_to_process))\n",
    "    features = pd.DataFrame(features)\n",
    "    features.columns = get_cols()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edca04d-3afe-4ed7-a620-5ad8308954c0",
   "metadata": {},
   "source": [
    "# 1) Creating improved features\n",
    "- In the previous notebooks I **retrieved the FO from the full audio file** but **extracted the MFCC from the middle segment of the file** with n seconds length. I **improve the validity of the extracted features by calculating the FO from the exact same audio segment as the MFCC.** This will degrade model accuracy somewhat since the FO is calculated based on much less information. At the same time this procedure is **more representative in regard to the podcast analysis** where I will cut episodes in segments of n seconds length. \n",
    "- I improved the extraction speed by loading the file just once and calculating FO and MFCC in one go.\n",
    "- I also changed the aubio algorithm for FO pitch extraction from `yin` to `yinfast` that yields almost identical results while being at least 2x faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0158e2f-6d6c-455d-b3fc-61edae2a3d4d",
   "metadata": {},
   "source": [
    "## Extracting improved features for CommonVoice\n",
    "Creating improved features for all languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b0a627b-7857-4111-954b-e4c1d09d11e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# languages = [\"_german\", \"_french\", \"_italian\", \"_romansh\"]\n",
    "\n",
    "# for language in languages:\n",
    "#     tmp = pd.read_csv(f\"_data/CommonVoice/{language}/validated.tsv\", sep=\"\\t\")\n",
    "#     meta = prepare_metadata(tmp, language)\n",
    "#     feat = process_audio_files(meta.file_path)\n",
    "    \n",
    "#     # dropping recordings for which FO or MFCC couldn't be computed \n",
    "#     feat.dropna(inplace=True)\n",
    "#     meta = meta.loc[feat.index]\n",
    "\n",
    "#     meta.reset_index(drop=True, inplace=True)\n",
    "#     feat.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "#     meta.to_parquet(f\"_saved_features/cv{language}_meta.parq\")\n",
    "#     feat.to_parquet(f\"_saved_features/cv{language}_features.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f28c550f-c042-4433-8270-8a81156ea28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load single language\n",
    "# language = \"_german\"\n",
    "# meta = pd.read_parquet(f\"_saved_features/cv{language}_meta.parq\")\n",
    "# feat = pd.read_parquet(f\"_saved_features/cv{language}_features.parq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045a99a5-7b73-4ac0-a02b-8141a79bb90a",
   "metadata": {},
   "source": [
    "From experiments I found that **I get slightly better results if I train on all languages used in Switzerland.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69e1a244-fb05-4321-87ec-1d0296c12c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load features for multiple languages\n",
    "metas = []\n",
    "feats = []\n",
    "\n",
    "languages = [\"_german\", \"_french\", \"_italian\", \"_romansh\"]\n",
    "for language in languages:\n",
    "    metas.append(pd.read_parquet(f\"_saved_features/cv{language}_meta.parq\"))\n",
    "    feats.append(pd.read_parquet(f\"_saved_features/cv{language}_features.parq\"))\n",
    "\n",
    "meta = pd.concat(metas).reset_index(drop=True)\n",
    "feat = pd.concat(feats).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dbd63a-c822-463b-8d27-627b6629ca1d",
   "metadata": {},
   "source": [
    "# 2) Creating an additional test data set from podcasts\n",
    "- **Until now I only examined and trained on CommonVoice data.** This was practical in order to analyze gender prediction. At the same time **the CommonVoice data is not fully representative in regards to the target domain of podcast media.**\n",
    "- To more thoroughly test the accuracy of the trained models I **created an additional data set derived from actual podcasts episodes and with  Swissgerman samples.**\n",
    "- I used unsupervised learning with KMeans to quickly cluster voice samples from various podcast episodes. I checked these samples manually and fixed erroneous samples. \n",
    "- **The podcast test set contains 2'200 audio samples** with 1'400 recordings in German, 650 in Swissgerman, 100 Italian and 50 English ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7d39557-0d08-4a4b-97c5-3a3005e95d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------  ----\n",
      "male    1100\n",
      "female  1100\n",
      "------  ----\n",
      "\n",
      "Languages in podcast test set:\n",
      "--  ----\n",
      "de  1400\n",
      "ch   650\n",
      "it   100\n",
      "en    50\n",
      "--  ----\n"
     ]
    }
   ],
   "source": [
    "podcast_samples = glob.glob(\"_data/_podcasts/podcasts_testdata/*/*\")\n",
    "meta_pod = pd.DataFrame([x.split(\"/\")[3:] for x in podcast_samples], columns=[\"gender\", \"file_name\"])\n",
    "meta_pod[\"file_path\"] = \"_data/_podcasts/podcasts_testdata/\" + meta_pod.gender + \"/\" + meta_pod.file_name\n",
    "meta_pod[\"language\"] = meta_pod.file_name.apply(lambda x: x.split(\"_\")[0])\n",
    "cols = ['gender', 'language', 'file_name', 'file_path']\n",
    "meta_pod = meta_pod[cols]\n",
    "\n",
    "print(tabulate(meta_pod.gender.value_counts().to_frame()))\n",
    "print()\n",
    "print(\"Languages in podcast test set:\")\n",
    "print(tabulate(meta_pod.language.value_counts().to_frame()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "975099ac-e633-48f0-bdff-2782ded8f756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045e963184864da5bfa62df1abf993c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# retrieving audio features for podcast test set\n",
    "feat_pod = process_audio_files(meta_pod.file_path)\n",
    "\n",
    "meta_pod.to_parquet(\"_saved_features/meta_pod.parq\")\n",
    "feat_pod.to_parquet(\"_saved_features/feat_pod.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54d71675-52bb-4024-918e-f858d4ddcf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------  ----\n",
      "male    1100\n",
      "female  1100\n",
      "------  ----\n",
      "\n",
      "Languages in podcast test set:\n",
      "--  ----\n",
      "de  1400\n",
      "ch   650\n",
      "it   100\n",
      "en    50\n",
      "--  ----\n"
     ]
    }
   ],
   "source": [
    "meta_pod = pd.read_parquet(\"_saved_features/meta_pod.parq\")\n",
    "feat_pod = pd.read_parquet(\"_saved_features/feat_pod.parq\")\n",
    "\n",
    "print(tabulate(meta_pod.gender.value_counts().to_frame()))\n",
    "print()\n",
    "print(\"Languages in podcast test set:\")\n",
    "print(tabulate(meta_pod.language.value_counts().to_frame()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0dfe20-375d-4b98-92a4-4bf324826bc1",
   "metadata": {},
   "source": [
    "# 3) Training models on improved features\n",
    "I do again a gridsearch with the most promising classifiers that I found in the previous step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "774bbe99-77d6-4a01-a200-34ddaf1ccaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feat\n",
    "y_train = meta.gender\n",
    "X_test = feat_pod\n",
    "y_test = meta_pod.gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3872e04-9856-475a-9266-024e040429d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_grid = []\n",
    "results_best = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ebe43a7-4b0f-4115-8096-ad969bef66d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_classifier(params_grid):\n",
    "    pipe = Pipeline(steps=[('scaler', StandardScaler()), \n",
    "                           ('estimator', LogisticRegression())])\n",
    "\n",
    "    grid = GridSearchCV(pipe, params_grid, n_jobs=-1, verbose=3)\n",
    "    grid.fit(X_train, y_train)\n",
    "    accuracy_validated = grid.best_score_\n",
    "    \n",
    "    y_pred = grid.predict(X_test)\n",
    "    accuracy_test_set = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print(f\"{accuracy_validated:.3f} accuracy CV 5fold\")\n",
    "    print(f\"{accuracy_test_set:.3f} accuracy on podcast test set\\n\")\n",
    "    \n",
    "    results_detailed = pd.DataFrame(grid.cv_results_)\n",
    "    cols = list(params_grid[0].keys())\n",
    "    cols = [\"param_\" + x for x in cols]\n",
    "    cols = sorted(cols)\n",
    "    cols.extend(['mean_test_score', 'std_test_score', 'rank_test_score', \"mean_fit_time\"])\n",
    "    results_detailed = results_detailed.loc[:, cols]\n",
    "    results_detailed.param_estimator = results_detailed.param_estimator.apply(lambda x: str(x).split(\"(\")[0])\n",
    "    results_detailed.columns = results_detailed.columns.str.replace(\"param_\", \"\")\n",
    "\n",
    "    display(results_detailed.sort_values(\"mean_test_score\", ascending=False)\n",
    "           .style\n",
    "           .highlight_max(color=\"lightgreen\", subset=[\"mean_test_score\"])\n",
    "           .highlight_min(color=\"lightgreen\", subset=[\"std_test_score\", \"rank_test_score\"])\n",
    "           .background_gradient(cmap='Reds', subset=['mean_fit_time'])\n",
    "           .hide_index()\n",
    "           .set_precision(3)\n",
    "           )\n",
    "    print()\n",
    "    print(\"Best parameters are\")\n",
    "    print(grid.best_params_)\n",
    "    print(\"-\"*80)\n",
    "    print()\n",
    "    \n",
    "    return grid, results_detailed, accuracy_validated, accuracy_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12db3d60-9097-4a57-9c9d-f7fed50e7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LogisticRegression(max_iter=1e3),\n",
    "    KNeighborsClassifier(),\n",
    "    SVC(kernel=\"rbf\"),\n",
    "    lgb.LGBMClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5619e99a-4af5-43a7-aca9-12c4167bc36f",
   "metadata": {},
   "source": [
    "- **All 5 classifiers yield good results crossvalidated (tested on the CommonVoice samples) in a close range of 0.933 to 0.948 accuracy.**\n",
    "- **SVC ranks best closely followed by LightGBM and Logistic regression.** \n",
    "- Unfortunately **SVC trains very slowly** (it'll be even slower when I train with `probability=True`).\n",
    "- Since Logistic Regression trains very fast with good results I examine this classifier more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "599947ef-ebfc-4957-b9eb-506ae550379e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "--------------------------------------------------------------------------------\n",
      "0.948 accuracy CV 5fold\n",
      "0.926 accuracy on podcast test set\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_b780b_row0_col2,#T_b780b_row0_col4,#T_b780b_row9_col3{\n",
       "            background-color:  lightgreen;\n",
       "        }#T_b780b_row0_col5{\n",
       "            background-color:  #67000d;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b780b_row1_col5{\n",
       "            background-color:  #dc2924;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b780b_row2_col5,#T_b780b_row3_col5{\n",
       "            background-color:  #fff3ed;\n",
       "            color:  #000000;\n",
       "        }#T_b780b_row4_col5,#T_b780b_row8_col5,#T_b780b_row9_col5{\n",
       "            background-color:  #fff5f0;\n",
       "            color:  #000000;\n",
       "        }#T_b780b_row5_col5,#T_b780b_row6_col5,#T_b780b_row7_col5{\n",
       "            background-color:  #fff4ef;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_b780b_\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >estimator</th>        <th class=\"col_heading level0 col1\" >scaler</th>        <th class=\"col_heading level0 col2\" >mean_test_score</th>        <th class=\"col_heading level0 col3\" >std_test_score</th>        <th class=\"col_heading level0 col4\" >rank_test_score</th>        <th class=\"col_heading level0 col5\" >mean_fit_time</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_b780b_row0_col0\" class=\"data row0 col0\" >SVC</td>\n",
       "                        <td id=\"T_b780b_row0_col1\" class=\"data row0 col1\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b780b_row0_col2\" class=\"data row0 col2\" >0.948</td>\n",
       "                        <td id=\"T_b780b_row0_col3\" class=\"data row0 col3\" >0.008</td>\n",
       "                        <td id=\"T_b780b_row0_col4\" class=\"data row0 col4\" >1</td>\n",
       "                        <td id=\"T_b780b_row0_col5\" class=\"data row0 col5\" >364.563</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b780b_row1_col0\" class=\"data row1 col0\" >SVC</td>\n",
       "                        <td id=\"T_b780b_row1_col1\" class=\"data row1 col1\" >QuantileTransformer()</td>\n",
       "                        <td id=\"T_b780b_row1_col2\" class=\"data row1 col2\" >0.947</td>\n",
       "                        <td id=\"T_b780b_row1_col3\" class=\"data row1 col3\" >0.007</td>\n",
       "                        <td id=\"T_b780b_row1_col4\" class=\"data row1 col4\" >2</td>\n",
       "                        <td id=\"T_b780b_row1_col5\" class=\"data row1 col5\" >251.699</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b780b_row2_col0\" class=\"data row2 col0\" >LGBMClassifier</td>\n",
       "                        <td id=\"T_b780b_row2_col1\" class=\"data row2 col1\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b780b_row2_col2\" class=\"data row2 col2\" >0.946</td>\n",
       "                        <td id=\"T_b780b_row2_col3\" class=\"data row2 col3\" >0.005</td>\n",
       "                        <td id=\"T_b780b_row2_col4\" class=\"data row2 col4\" >3</td>\n",
       "                        <td id=\"T_b780b_row2_col5\" class=\"data row2 col5\" >4.493</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b780b_row3_col0\" class=\"data row3 col0\" >LGBMClassifier</td>\n",
       "                        <td id=\"T_b780b_row3_col1\" class=\"data row3 col1\" >QuantileTransformer()</td>\n",
       "                        <td id=\"T_b780b_row3_col2\" class=\"data row3 col2\" >0.945</td>\n",
       "                        <td id=\"T_b780b_row3_col3\" class=\"data row3 col3\" >0.005</td>\n",
       "                        <td id=\"T_b780b_row3_col4\" class=\"data row3 col4\" >4</td>\n",
       "                        <td id=\"T_b780b_row3_col5\" class=\"data row3 col5\" >5.266</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b780b_row4_col0\" class=\"data row4 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b780b_row4_col1\" class=\"data row4 col1\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b780b_row4_col2\" class=\"data row4 col2\" >0.943</td>\n",
       "                        <td id=\"T_b780b_row4_col3\" class=\"data row4 col3\" >0.005</td>\n",
       "                        <td id=\"T_b780b_row4_col4\" class=\"data row4 col4\" >5</td>\n",
       "                        <td id=\"T_b780b_row4_col5\" class=\"data row4 col5\" >0.734</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b780b_row5_col0\" class=\"data row5 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b780b_row5_col1\" class=\"data row5 col1\" >QuantileTransformer()</td>\n",
       "                        <td id=\"T_b780b_row5_col2\" class=\"data row5 col2\" >0.943</td>\n",
       "                        <td id=\"T_b780b_row5_col3\" class=\"data row5 col3\" >0.005</td>\n",
       "                        <td id=\"T_b780b_row5_col4\" class=\"data row5 col4\" >6</td>\n",
       "                        <td id=\"T_b780b_row5_col5\" class=\"data row5 col5\" >2.815</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b780b_row6_col0\" class=\"data row6 col0\" >KNeighborsClassifier</td>\n",
       "                        <td id=\"T_b780b_row6_col1\" class=\"data row6 col1\" >QuantileTransformer()</td>\n",
       "                        <td id=\"T_b780b_row6_col2\" class=\"data row6 col2\" >0.936</td>\n",
       "                        <td id=\"T_b780b_row6_col3\" class=\"data row6 col3\" >0.011</td>\n",
       "                        <td id=\"T_b780b_row6_col4\" class=\"data row6 col4\" >7</td>\n",
       "                        <td id=\"T_b780b_row6_col5\" class=\"data row6 col5\" >1.642</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b780b_row7_col0\" class=\"data row7 col0\" >QuadraticDiscriminantAnalysis</td>\n",
       "                        <td id=\"T_b780b_row7_col1\" class=\"data row7 col1\" >QuantileTransformer()</td>\n",
       "                        <td id=\"T_b780b_row7_col2\" class=\"data row7 col2\" >0.935</td>\n",
       "                        <td id=\"T_b780b_row7_col3\" class=\"data row7 col3\" >0.006</td>\n",
       "                        <td id=\"T_b780b_row7_col4\" class=\"data row7 col4\" >8</td>\n",
       "                        <td id=\"T_b780b_row7_col5\" class=\"data row7 col5\" >2.416</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b780b_row8_col0\" class=\"data row8 col0\" >KNeighborsClassifier</td>\n",
       "                        <td id=\"T_b780b_row8_col1\" class=\"data row8 col1\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b780b_row8_col2\" class=\"data row8 col2\" >0.935</td>\n",
       "                        <td id=\"T_b780b_row8_col3\" class=\"data row8 col3\" >0.009</td>\n",
       "                        <td id=\"T_b780b_row8_col4\" class=\"data row8 col4\" >9</td>\n",
       "                        <td id=\"T_b780b_row8_col5\" class=\"data row8 col5\" >0.203</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b780b_row9_col0\" class=\"data row9 col0\" >QuadraticDiscriminantAnalysis</td>\n",
       "                        <td id=\"T_b780b_row9_col1\" class=\"data row9 col1\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b780b_row9_col2\" class=\"data row9 col2\" >0.933</td>\n",
       "                        <td id=\"T_b780b_row9_col3\" class=\"data row9 col3\" >0.004</td>\n",
       "                        <td id=\"T_b780b_row9_col4\" class=\"data row9 col4\" >10</td>\n",
       "                        <td id=\"T_b780b_row9_col5\" class=\"data row9 col5\" >1.386</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17fa56280>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters are\n",
      "{'estimator': SVC(), 'scaler': StandardScaler()}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "CPU times: user 3min 47s, sys: 1.88 s, total: 3min 49s\n",
      "Wall time: 13min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params_grid = [{\"scaler\": [StandardScaler(), \n",
    "                           QuantileTransformer()],\n",
    "                'estimator': classifiers,\n",
    "               }]\n",
    "\n",
    "_, _, _, _= gridsearch_classifier(params_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20121a4d-82d2-4ed5-b31b-50e87cbb807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "--------------------------------------------------------------------------------\n",
      "0.944 accuracy CV 5fold\n",
      "0.926 accuracy on podcast test set\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_b6a6c_row0_col3,#T_b6a6c_row0_col5,#T_b6a6c_row16_col4{\n",
       "            background-color:  lightgreen;\n",
       "        }#T_b6a6c_row0_col6{\n",
       "            background-color:  #980c13;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b6a6c_row1_col6{\n",
       "            background-color:  #e12d26;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b6a6c_row2_col6{\n",
       "            background-color:  #67000d;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b6a6c_row3_col6{\n",
       "            background-color:  #a30f15;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b6a6c_row4_col6,#T_b6a6c_row14_col6{\n",
       "            background-color:  #b71319;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b6a6c_row5_col6{\n",
       "            background-color:  #820711;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b6a6c_row6_col6{\n",
       "            background-color:  #b91419;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b6a6c_row7_col6{\n",
       "            background-color:  #fff5f0;\n",
       "            color:  #000000;\n",
       "        }#T_b6a6c_row8_col6,#T_b6a6c_row15_col6{\n",
       "            background-color:  #9d0d14;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b6a6c_row9_col6{\n",
       "            background-color:  #bb141a;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b6a6c_row10_col6{\n",
       "            background-color:  #ad1117;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b6a6c_row11_col6{\n",
       "            background-color:  #c8171c;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b6a6c_row12_col6{\n",
       "            background-color:  #a91016;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b6a6c_row13_col6{\n",
       "            background-color:  #f14130;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b6a6c_row16_col6{\n",
       "            background-color:  #840711;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b6a6c_row17_col6{\n",
       "            background-color:  #d82422;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_b6a6c_row18_col6{\n",
       "            background-color:  #fb7757;\n",
       "            color:  #000000;\n",
       "        }#T_b6a6c_row19_col6{\n",
       "            background-color:  #fdd7c6;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_b6a6c_\" ><thead>    <tr>        <th class=\"col_heading level0 col0\" >estimator</th>        <th class=\"col_heading level0 col1\" >estimator__C</th>        <th class=\"col_heading level0 col2\" >scaler</th>        <th class=\"col_heading level0 col3\" >mean_test_score</th>        <th class=\"col_heading level0 col4\" >std_test_score</th>        <th class=\"col_heading level0 col5\" >rank_test_score</th>        <th class=\"col_heading level0 col6\" >mean_fit_time</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_b6a6c_row0_col0\" class=\"data row0 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row0_col1\" class=\"data row0 col1\" >0.336</td>\n",
       "                        <td id=\"T_b6a6c_row0_col2\" class=\"data row0 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row0_col3\" class=\"data row0 col3\" >0.944</td>\n",
       "                        <td id=\"T_b6a6c_row0_col4\" class=\"data row0 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row0_col5\" class=\"data row0 col5\" >1</td>\n",
       "                        <td id=\"T_b6a6c_row0_col6\" class=\"data row0 col6\" >0.837</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row1_col0\" class=\"data row1 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row1_col1\" class=\"data row1 col1\" >0.162</td>\n",
       "                        <td id=\"T_b6a6c_row1_col2\" class=\"data row1 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row1_col3\" class=\"data row1 col3\" >0.944</td>\n",
       "                        <td id=\"T_b6a6c_row1_col4\" class=\"data row1 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row1_col5\" class=\"data row1 col5\" >2</td>\n",
       "                        <td id=\"T_b6a6c_row1_col6\" class=\"data row1 col6\" >0.800</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row2_col0\" class=\"data row2 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row2_col1\" class=\"data row2 col1\" >0.207</td>\n",
       "                        <td id=\"T_b6a6c_row2_col2\" class=\"data row2 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row2_col3\" class=\"data row2 col3\" >0.944</td>\n",
       "                        <td id=\"T_b6a6c_row2_col4\" class=\"data row2 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row2_col5\" class=\"data row2 col5\" >3</td>\n",
       "                        <td id=\"T_b6a6c_row2_col6\" class=\"data row2 col6\" >0.854</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row3_col0\" class=\"data row3 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row3_col1\" class=\"data row3 col1\" >0.264</td>\n",
       "                        <td id=\"T_b6a6c_row3_col2\" class=\"data row3 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row3_col3\" class=\"data row3 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row3_col4\" class=\"data row3 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row3_col5\" class=\"data row3 col5\" >4</td>\n",
       "                        <td id=\"T_b6a6c_row3_col6\" class=\"data row3 col6\" >0.834</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row4_col0\" class=\"data row4 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row4_col1\" class=\"data row4 col1\" >0.546</td>\n",
       "                        <td id=\"T_b6a6c_row4_col2\" class=\"data row4 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row4_col3\" class=\"data row4 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row4_col4\" class=\"data row4 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row4_col5\" class=\"data row4 col5\" >5</td>\n",
       "                        <td id=\"T_b6a6c_row4_col6\" class=\"data row4 col6\" >0.823</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row5_col0\" class=\"data row5 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row5_col1\" class=\"data row5 col1\" >1.833</td>\n",
       "                        <td id=\"T_b6a6c_row5_col2\" class=\"data row5 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row5_col3\" class=\"data row5 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row5_col4\" class=\"data row5 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row5_col5\" class=\"data row5 col5\" >6</td>\n",
       "                        <td id=\"T_b6a6c_row5_col6\" class=\"data row5 col6\" >0.845</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row6_col0\" class=\"data row6 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row6_col1\" class=\"data row6 col1\" >0.428</td>\n",
       "                        <td id=\"T_b6a6c_row6_col2\" class=\"data row6 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row6_col3\" class=\"data row6 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row6_col4\" class=\"data row6 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row6_col5\" class=\"data row6 col5\" >7</td>\n",
       "                        <td id=\"T_b6a6c_row6_col6\" class=\"data row6 col6\" >0.822</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row7_col0\" class=\"data row7 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row7_col1\" class=\"data row7 col1\" >10.000</td>\n",
       "                        <td id=\"T_b6a6c_row7_col2\" class=\"data row7 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row7_col3\" class=\"data row7 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row7_col4\" class=\"data row7 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row7_col5\" class=\"data row7 col5\" >8</td>\n",
       "                        <td id=\"T_b6a6c_row7_col6\" class=\"data row7 col6\" >0.689</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row8_col0\" class=\"data row8 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row8_col1\" class=\"data row8 col1\" >0.695</td>\n",
       "                        <td id=\"T_b6a6c_row8_col2\" class=\"data row8 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row8_col3\" class=\"data row8 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row8_col4\" class=\"data row8 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row8_col5\" class=\"data row8 col5\" >9</td>\n",
       "                        <td id=\"T_b6a6c_row8_col6\" class=\"data row8 col6\" >0.835</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row9_col0\" class=\"data row9 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row9_col1\" class=\"data row9 col1\" >7.848</td>\n",
       "                        <td id=\"T_b6a6c_row9_col2\" class=\"data row9 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row9_col3\" class=\"data row9 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row9_col4\" class=\"data row9 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row9_col5\" class=\"data row9 col5\" >10</td>\n",
       "                        <td id=\"T_b6a6c_row9_col6\" class=\"data row9 col6\" >0.822</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row10_col0\" class=\"data row10 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row10_col1\" class=\"data row10 col1\" >6.158</td>\n",
       "                        <td id=\"T_b6a6c_row10_col2\" class=\"data row10 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row10_col3\" class=\"data row10 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row10_col4\" class=\"data row10 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row10_col5\" class=\"data row10 col5\" >10</td>\n",
       "                        <td id=\"T_b6a6c_row10_col6\" class=\"data row10 col6\" >0.828</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row11_col0\" class=\"data row11 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row11_col1\" class=\"data row11 col1\" >4.833</td>\n",
       "                        <td id=\"T_b6a6c_row11_col2\" class=\"data row11 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row11_col3\" class=\"data row11 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row11_col4\" class=\"data row11 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row11_col5\" class=\"data row11 col5\" >10</td>\n",
       "                        <td id=\"T_b6a6c_row11_col6\" class=\"data row11 col6\" >0.814</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row12_col0\" class=\"data row12 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row12_col1\" class=\"data row12 col1\" >2.336</td>\n",
       "                        <td id=\"T_b6a6c_row12_col2\" class=\"data row12 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row12_col3\" class=\"data row12 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row12_col4\" class=\"data row12 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row12_col5\" class=\"data row12 col5\" >13</td>\n",
       "                        <td id=\"T_b6a6c_row12_col6\" class=\"data row12 col6\" >0.831</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row13_col0\" class=\"data row13 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row13_col1\" class=\"data row13 col1\" >1.438</td>\n",
       "                        <td id=\"T_b6a6c_row13_col2\" class=\"data row13 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row13_col3\" class=\"data row13 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row13_col4\" class=\"data row13 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row13_col5\" class=\"data row13 col5\" >14</td>\n",
       "                        <td id=\"T_b6a6c_row13_col6\" class=\"data row13 col6\" >0.789</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row14_col0\" class=\"data row14 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row14_col1\" class=\"data row14 col1\" >3.793</td>\n",
       "                        <td id=\"T_b6a6c_row14_col2\" class=\"data row14 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row14_col3\" class=\"data row14 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row14_col4\" class=\"data row14 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row14_col5\" class=\"data row14 col5\" >15</td>\n",
       "                        <td id=\"T_b6a6c_row14_col6\" class=\"data row14 col6\" >0.823</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row15_col0\" class=\"data row15 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row15_col1\" class=\"data row15 col1\" >2.976</td>\n",
       "                        <td id=\"T_b6a6c_row15_col2\" class=\"data row15 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row15_col3\" class=\"data row15 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row15_col4\" class=\"data row15 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row15_col5\" class=\"data row15 col5\" >16</td>\n",
       "                        <td id=\"T_b6a6c_row15_col6\" class=\"data row15 col6\" >0.836</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row16_col0\" class=\"data row16 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row16_col1\" class=\"data row16 col1\" >0.886</td>\n",
       "                        <td id=\"T_b6a6c_row16_col2\" class=\"data row16 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row16_col3\" class=\"data row16 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row16_col4\" class=\"data row16 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row16_col5\" class=\"data row16 col5\" >17</td>\n",
       "                        <td id=\"T_b6a6c_row16_col6\" class=\"data row16 col6\" >0.844</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row17_col0\" class=\"data row17 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row17_col1\" class=\"data row17 col1\" >1.129</td>\n",
       "                        <td id=\"T_b6a6c_row17_col2\" class=\"data row17 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row17_col3\" class=\"data row17 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row17_col4\" class=\"data row17 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row17_col5\" class=\"data row17 col5\" >18</td>\n",
       "                        <td id=\"T_b6a6c_row17_col6\" class=\"data row17 col6\" >0.805</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row18_col0\" class=\"data row18 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row18_col1\" class=\"data row18 col1\" >0.127</td>\n",
       "                        <td id=\"T_b6a6c_row18_col2\" class=\"data row18 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row18_col3\" class=\"data row18 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row18_col4\" class=\"data row18 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row18_col5\" class=\"data row18 col5\" >18</td>\n",
       "                        <td id=\"T_b6a6c_row18_col6\" class=\"data row18 col6\" >0.764</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_b6a6c_row19_col0\" class=\"data row19 col0\" >LogisticRegression</td>\n",
       "                        <td id=\"T_b6a6c_row19_col1\" class=\"data row19 col1\" >0.100</td>\n",
       "                        <td id=\"T_b6a6c_row19_col2\" class=\"data row19 col2\" >StandardScaler()</td>\n",
       "                        <td id=\"T_b6a6c_row19_col3\" class=\"data row19 col3\" >0.943</td>\n",
       "                        <td id=\"T_b6a6c_row19_col4\" class=\"data row19 col4\" >0.005</td>\n",
       "                        <td id=\"T_b6a6c_row19_col5\" class=\"data row19 col5\" >20</td>\n",
       "                        <td id=\"T_b6a6c_row19_col6\" class=\"data row19 col6\" >0.715</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17fb7b1c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters are\n",
      "{'estimator': LogisticRegression(C=0.33598182862837817, max_iter=1000.0), 'estimator__C': 0.33598182862837817, 'scaler': StandardScaler()}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "CPU times: user 2.78 s, sys: 697 ms, total: 3.48 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params_grid = [{\"scaler\": [StandardScaler()],\n",
    "                'estimator':[LogisticRegression(max_iter=1e3)],\n",
    "                'estimator__C': np.logspace(-1, 1, 20),\n",
    "                }\n",
    "              ]\n",
    "\n",
    "_, _, _, _ = gridsearch_classifier(params_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5c587a-dacc-47a7-a016-29be117c75e6",
   "metadata": {},
   "source": [
    "# 4) Evaluating Logistic regression trained with the improved features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50054e50-eecd-40c3-b84e-2b4b0e37f057",
   "metadata": {},
   "source": [
    "Since Logistic regression trains fast and yields very good results I examine its predictions in greater detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6be8b1df-1eab-475b-9c71-01e64e69c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_test, y_test, cv_folds=5,\n",
    "                clf=LogisticRegression(max_iter=1e3, C=0.4),\n",
    "                report=True):\n",
    "    \n",
    "    pipe = make_pipeline(QuantileTransformer(), clf)   \n",
    "    \n",
    "    scores = cross_val_score(pipe, X_train, y_train, cv=cv_folds, n_jobs=-1)\n",
    "    print(f\"{np.mean(scores):.3f} accuracy crossvalidated {cv_folds}fold\")\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    print(f\"{accuracy_score(y_test, y_pred):.3f} accuracy on test set\")\n",
    "    print()\n",
    "    \n",
    "    if report == True:\n",
    "        print(classification_report(y_test, y_pred, zero_division=False))\n",
    "\n",
    "        cfmtrx = confusion_matrix(y_test, y_pred, normalize=None)\n",
    "        display(pd.DataFrame(cfmtrx, \n",
    "                     index=[x + \"_true\" for x in [\"female\", \"male\"]],\n",
    "                     columns=[x + \"_pred\" for x in [\"female\", \"male\"]]\n",
    "                    ))\n",
    "        \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede99fa2-dbfe-49f4-a7ed-8e537b6d610b",
   "metadata": {},
   "source": [
    "- Logistic regression yields close to 0.93 accuracy.\n",
    "- Precision for male voices is a little less good (on this small test data set). This is due to many samples of two female voices being missclassified (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c10d7a86-03e6-4dc0-91b1-83f8659c0724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.943 accuracy crossvalidated 5fold\n",
      "0.926 accuracy on test set\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.94      0.91      0.92      1100\n",
      "        male       0.91      0.94      0.93      1100\n",
      "\n",
      "    accuracy                           0.93      2200\n",
      "   macro avg       0.93      0.93      0.93      2200\n",
      "weighted avg       0.93      0.93      0.93      2200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female_pred</th>\n",
       "      <th>male_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female_true</th>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male_true</th>\n",
       "      <td>63</td>\n",
       "      <td>1037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             female_pred  male_pred\n",
       "female_true         1000        100\n",
       "male_true             63       1037"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = train_model(X_train, y_train, \n",
    "                   X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc32dc85-d189-4fad-aa06-7704094fbb49",
   "metadata": {},
   "source": [
    "From **checking the wrong predictions of the classifier** I gather:\n",
    "- **Many false predictions seem comprehensible.**\n",
    "- **Almost all samples of two female speakers are wrongly predicted as male samples** (both have particularly deep voices). These errors alone account for around 4% of the 7% total errors.\n",
    "- Several missclassified male speakers have particularly playful, energetic and expressive voices, that seem higher pitched too.\n",
    "- Many of the Italian male samples are missclassified (again: very playful and high pitch vocal expression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14e88d01-4054-4639-a3a4-a313241f2b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = meta_pod.copy()\n",
    "# preds = pipe.predict(X_test)\n",
    "# preds_bool = [True if x==\"male\" else False for x in preds]\n",
    "# proba = pipe.predict_proba(X_test)\n",
    "# tmp[\"predicted\"] = preds\n",
    "# tmp[\"predicted_proba\"] = proba.max(axis=1)\n",
    "# cols = ['language', 'file_name', 'gender', 'predicted']\n",
    "# display(tmp[cols][tmp.gender!=tmp.predicted].sort_values(\"file_name\")\n",
    "#        .style\n",
    "#        .hide_index())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
